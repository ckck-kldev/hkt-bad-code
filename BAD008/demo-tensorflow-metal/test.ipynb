{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>   AI-Benchmark-v.0.1.2   \n",
      ">>   Let the AI Games begin..\n",
      "\n",
      "Metal device set to: Apple M2 Max\n",
      "\n",
      "systemMemory: 32.00 GB\n",
      "maxCacheSize: 10.67 GB\n",
      "\n",
      "*  TF Version: 2.12.0\n",
      "*  Platform: macOS-13.3.1-arm64-arm-64bit\n",
      "*  CPU: N/A\n",
      "*  CPU RAM: 32 GB\n",
      "*  GPU/0: N/A\n",
      "*  GPU RAM: N/A GB\n",
      "*  CUDA Version: N/A\n",
      "*  CUDA Build: N/A\n",
      "\n",
      "The benchmark is running...\n",
      "The tests might take up to 20 minutes\n",
      "Please don't interrupt the script\n",
      "\n",
      "1/19. MobileNet-V2\n",
      "\n",
      "1.1 - inference | batch=50, size=224x224: 315 ± 9 ms\n",
      "1.2 - training  | batch=50, size=224x224: 885 ± 25 ms\n",
      "\n",
      "2/19. Inception-V3\n",
      "\n",
      "2.1 - inference | batch=20, size=346x346: 316 ± 31 ms\n",
      "2.2 - training  | batch=20, size=346x346: 875 ± 57 ms\n",
      "\n",
      "3/19. Inception-V4\n",
      "\n",
      "3.1 - inference | batch=10, size=346x346: 374 ± 32 ms\n",
      "3.2 - training  | batch=10, size=346x346: 956 ± 67 ms\n",
      "\n",
      "4/19. Inception-ResNet-V2\n",
      "\n",
      "4.1 - inference | batch=10, size=346x346: 501 ± 85 ms\n",
      "4.2 - training  | batch=8, size=346x346: 1497 ± 233 ms\n",
      "\n",
      "5/19. ResNet-V2-50\n",
      "\n",
      "5.1 - inference | batch=10, size=346x346: 272 ± 54 ms\n",
      "5.2 - training  | batch=10, size=346x346: 760 ± 114 ms\n",
      "\n",
      "6/19. ResNet-V2-152\n",
      "\n",
      "6.1 - inference | batch=10, size=256x256: 483 ± 167 ms\n",
      "6.2 - training  | batch=10, size=256x256: 1711 ± 349 ms\n",
      "\n",
      "7/19. VGG-16\n",
      "\n",
      "7.1 - inference | batch=20, size=224x224: 119 ± 4 ms\n",
      "7.2 - training  | batch=2, size=224x224: 171 ± 56 ms\n",
      "\n",
      "8/19. SRCNN 9-5-5\n",
      "\n",
      "8.1 - inference | batch=10, size=512x512: 98.4 ± 4.5 ms\n",
      "8.2 - inference | batch=1, size=1536x1536: 87.1 ± 2.9 ms\n",
      "8.3 - training  | batch=10, size=512x512: 297 ± 7 ms\n",
      "\n",
      "9/19. VGG-19 Super-Res\n",
      "\n",
      "9.1 - inference | batch=10, size=256x256: 121 ± 2 ms\n",
      "9.2 - inference | batch=1, size=1024x1024: 187 ± 3 ms\n",
      "9.3 - training  | batch=10, size=224x224: 295 ± 4 ms\n",
      "\n",
      "10/19. ResNet-SRGAN\n",
      "\n",
      "10.1 - inference | batch=10, size=512x512: 199 ± 63 ms\n",
      "10.2 - inference | batch=1, size=1536x1536: 144 ± 40 ms\n",
      "10.3 - training  | batch=5, size=512x512: 293 ± 101 ms\n",
      "\n",
      "11/19. ResNet-DPED\n",
      "\n",
      "11.1 - inference | batch=10, size=256x256: 159 ± 2 ms\n",
      "11.2 - inference | batch=1, size=1024x1024: 256 ± 17 ms\n",
      "11.3 - training  | batch=15, size=128x128: 260 ± 86 ms\n",
      "\n",
      "12/19. U-Net\n",
      "\n",
      "12.1 - inference | batch=4, size=512x512: 283 ± 12 ms\n",
      "12.2 - inference | batch=1, size=1024x1024: 302 ± 26 ms\n",
      "12.3 - training  | batch=4, size=256x256: 491 ± 204 ms\n",
      "\n",
      "13/19. Nvidia-SPADE\n",
      "\n",
      "13.1 - inference | batch=5, size=128x128: 391 ± 147 ms\n",
      "13.2 - training  | batch=1, size=128x128: 992 ± 282 ms\n",
      "\n",
      "14/19. ICNet\n",
      "\n",
      "14.1 - inference | batch=5, size=1024x1536: 401 ± 122 ms\n",
      "14.2 - training  | batch=10, size=1024x1536: 649 ± 124 ms\n",
      "\n",
      "15/19. PSPNet\n",
      "\n",
      "15.1 - inference | batch=5, size=720x720: 1957 ± 50 ms\n",
      "15.2 - training  | batch=1, size=512x512: 933 ± 134 ms\n",
      "\n",
      "16/19. DeepLab\n",
      "\n",
      "16.1 - inference | batch=2, size=512x512: 926 ± 168 ms\n",
      "16.2 - training  | batch=1, size=384x384: 1183 ± 218 ms\n",
      "\n",
      "17/19. Pixel-RNN\n",
      "\n",
      "17.1 - inference | batch=50, size=64x64: 22325 ± 377 ms\n",
      "17.2 - training  | batch=10, size=64x64: 331047.0 ± 0.0 ms\n",
      "\n",
      "18/19. LSTM-Sentiment\n",
      "\n",
      "18.1 - inference | batch=100, size=1024x300: 31593 ± 146 ms\n",
      "18.2 - training  | batch=10, size=1024x300: 964234.0 ± 0.0 ms\n",
      "\n",
      "19/19. GNMT-Translation\n",
      "\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Graph execution error:\n\nDetected at node 'dynamic_seq2seq/decoder/decoder/LogicalOr' defined at (most recent call last):\nNode: 'dynamic_seq2seq/decoder/decoder/LogicalOr'\nNo registered 'BroadcastTo' OpKernel for 'GPU' devices compatible with node {{node dynamic_seq2seq/decoder/decoder/LogicalOr}}\n\t (OpKernel was found, but attributes didn't match) Requested Attributes: T=DT_BOOL, Tidx=DT_INT32, _XlaHasReferenceVars=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"\n\t.  Registered:  device='XLA_CPU_JIT'; Tidx in [DT_INT32, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, 930109355527764061, DT_HALF, DT_UINT32, DT_UINT64, DT_FLOAT8_E5M2, DT_FLOAT8_E4M3FN]\n  device='GPU'; T in [DT_FLOAT]\n  device='DEFAULT'; T in [DT_INT32]\n  device='CPU'; T in [DT_UINT64]\n  device='CPU'; T in [DT_INT64]\n  device='CPU'; T in [DT_UINT32]\n  device='CPU'; T in [DT_UINT16]\n  device='CPU'; T in [DT_INT16]\n  device='CPU'; T in [DT_UINT8]\n  device='CPU'; T in [DT_INT8]\n  device='CPU'; T in [DT_INT32]\n  device='CPU'; T in [DT_HALF]\n  device='CPU'; T in [DT_BFLOAT16]\n  device='CPU'; T in [DT_FLOAT]\n  device='CPU'; T in [DT_DOUBLE]\n  device='CPU'; T in [DT_COMPLEX64]\n  device='CPU'; T in [DT_COMPLEX128]\n  device='CPU'; T in [DT_BOOL]\n  device='CPU'; T in [DT_STRING]\n  device='CPU'; T in [DT_RESOURCE]\n  device='CPU'; T in [DT_VARIANT]\n\n\t [[dynamic_seq2seq/decoder/decoder/LogicalOr]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/tensorflow/python/client/session.py:1378\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m   1379\u001b[0m \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mOpError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/tensorflow/python/client/session.py:1361\u001b[0m, in \u001b[0;36mBaseSession._do_run.<locals>._run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extend_graph()\n\u001b[0;32m-> 1361\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m   1362\u001b[0m                                 target_list, run_metadata)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/tensorflow/python/client/session.py:1454\u001b[0m, in \u001b[0;36mBaseSession._call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1452\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_tf_sessionrun\u001b[39m(\u001b[39mself\u001b[39m, options, feed_dict, fetch_list, target_list,\n\u001b[1;32m   1453\u001b[0m                         run_metadata):\n\u001b[0;32m-> 1454\u001b[0m   \u001b[39mreturn\u001b[39;00m tf_session\u001b[39m.\u001b[39;49mTF_SessionRun_wrapper(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_session, options, feed_dict,\n\u001b[1;32m   1455\u001b[0m                                           fetch_list, target_list,\n\u001b[1;32m   1456\u001b[0m                                           run_metadata)\n",
      "\u001b[0;31mNotFoundError\u001b[0m: No registered 'BroadcastTo' OpKernel for 'GPU' devices compatible with node {{node dynamic_seq2seq/decoder/decoder/LogicalOr}}\n\t (OpKernel was found, but attributes didn't match) Requested Attributes: T=DT_BOOL, Tidx=DT_INT32, _XlaHasReferenceVars=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"\n\t.  Registered:  device='XLA_CPU_JIT'; Tidx in [DT_INT32, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, 930109355527764061, DT_HALF, DT_UINT32, DT_UINT64, DT_FLOAT8_E5M2, DT_FLOAT8_E4M3FN]\n  device='GPU'; T in [DT_FLOAT]\n  device='DEFAULT'; T in [DT_INT32]\n  device='CPU'; T in [DT_UINT64]\n  device='CPU'; T in [DT_INT64]\n  device='CPU'; T in [DT_UINT32]\n  device='CPU'; T in [DT_UINT16]\n  device='CPU'; T in [DT_INT16]\n  device='CPU'; T in [DT_UINT8]\n  device='CPU'; T in [DT_INT8]\n  device='CPU'; T in [DT_INT32]\n  device='CPU'; T in [DT_HALF]\n  device='CPU'; T in [DT_BFLOAT16]\n  device='CPU'; T in [DT_FLOAT]\n  device='CPU'; T in [DT_DOUBLE]\n  device='CPU'; T in [DT_COMPLEX64]\n  device='CPU'; T in [DT_COMPLEX128]\n  device='CPU'; T in [DT_BOOL]\n  device='CPU'; T in [DT_STRING]\n  device='CPU'; T in [DT_RESOURCE]\n  device='CPU'; T in [DT_VARIANT]\n\n\t [[dynamic_seq2seq/decoder/decoder/LogicalOr]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mai_benchmark\u001b[39;00m \u001b[39mimport\u001b[39;00m AIBenchmark\n\u001b[1;32m      2\u001b[0m benchmark \u001b[39m=\u001b[39m AIBenchmark()\n\u001b[0;32m----> 3\u001b[0m results \u001b[39m=\u001b[39m benchmark\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/VSCode_local/tecky_lesson/hkt-bad-code/BAD008/demo-tensorflow-metal/metal_env/lib/python3.10/site-packages/ai_benchmark/__init__.py:63\u001b[0m, in \u001b[0;36mAIBenchmark.run\u001b[0;34m(self, precision)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m, precision\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnormal\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m---> 63\u001b[0m     \u001b[39mreturn\u001b[39;00m run_tests(training\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, inference\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, micro\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m     64\u001b[0m                      use_CPU\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muse_CPU, precision\u001b[39m=\u001b[39;49mprecision, _type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mfull\u001b[39;49m\u001b[39m\"\u001b[39;49m, start_dir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcwd)\n",
      "File \u001b[0;32m~/VSCode_local/tecky_lesson/hkt-bad-code/BAD008/demo-tensorflow-metal/metal_env/lib/python3.10/site-packages/ai_benchmark/utils.py:583\u001b[0m, in \u001b[0;36mrun_tests\u001b[0;34m(training, inference, micro, verbose, use_CPU, precision, _type, start_dir)\u001b[0m\n\u001b[1;32m    581\u001b[0m data \u001b[39m=\u001b[39m loadData(test\u001b[39m.\u001b[39mtype, subTest\u001b[39m.\u001b[39mgetInputDims())\n\u001b[1;32m    582\u001b[0m time_iter_started \u001b[39m=\u001b[39m getTimeMillis()\n\u001b[0;32m--> 583\u001b[0m sess\u001b[39m.\u001b[39;49mrun(output_, feed_dict\u001b[39m=\u001b[39;49m{input_: data})\n\u001b[1;32m    584\u001b[0m inference_time \u001b[39m=\u001b[39m getTimeMillis() \u001b[39m-\u001b[39m time_iter_started\n\u001b[1;32m    585\u001b[0m inference_times\u001b[39m.\u001b[39mappend(inference_time)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/tensorflow/python/client/session.py:968\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    965\u001b[0m run_metadata_ptr \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_NewBuffer() \u001b[39mif\u001b[39;00m run_metadata \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    967\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 968\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(\u001b[39mNone\u001b[39;49;00m, fetches, feed_dict, options_ptr,\n\u001b[1;32m    969\u001b[0m                      run_metadata_ptr)\n\u001b[1;32m    970\u001b[0m   \u001b[39mif\u001b[39;00m run_metadata:\n\u001b[1;32m    971\u001b[0m     proto_data \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/tensorflow/python/client/session.py:1191\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[39m# We only want to really perform the run if fetches or targets are provided,\u001b[39;00m\n\u001b[1;32m   1189\u001b[0m \u001b[39m# or if the call is a partial run that specifies feeds.\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m \u001b[39mif\u001b[39;00m final_fetches \u001b[39mor\u001b[39;00m final_targets \u001b[39mor\u001b[39;00m (handle \u001b[39mand\u001b[39;00m feed_dict_tensor):\n\u001b[0;32m-> 1191\u001b[0m   results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_run(handle, final_targets, final_fetches,\n\u001b[1;32m   1192\u001b[0m                          feed_dict_tensor, options, run_metadata)\n\u001b[1;32m   1193\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1194\u001b[0m   results \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/tensorflow/python/client/session.py:1371\u001b[0m, in \u001b[0;36mBaseSession._do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1368\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001b[1;32m   1370\u001b[0m \u001b[39mif\u001b[39;00m handle \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1371\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m   1372\u001b[0m                        run_metadata)\n\u001b[1;32m   1373\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/tensorflow/python/client/session.py:1397\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1392\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39monly supports NHWC tensor format\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m message:\n\u001b[1;32m   1393\u001b[0m   message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mA possible workaround: Try disabling Grappler optimizer\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1394\u001b[0m               \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mby modifying the config for creating the session eg.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1395\u001b[0m               \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39msession_config.graph_options.rewrite_options.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1396\u001b[0m               \u001b[39m'\u001b[39m\u001b[39mdisable_meta_optimizer = True\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1397\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mtype\u001b[39m(e)(node_def, op, message)\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Graph execution error:\n\nDetected at node 'dynamic_seq2seq/decoder/decoder/LogicalOr' defined at (most recent call last):\nNode: 'dynamic_seq2seq/decoder/decoder/LogicalOr'\nNo registered 'BroadcastTo' OpKernel for 'GPU' devices compatible with node {{node dynamic_seq2seq/decoder/decoder/LogicalOr}}\n\t (OpKernel was found, but attributes didn't match) Requested Attributes: T=DT_BOOL, Tidx=DT_INT32, _XlaHasReferenceVars=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"\n\t.  Registered:  device='XLA_CPU_JIT'; Tidx in [DT_INT32, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, 930109355527764061, DT_HALF, DT_UINT32, DT_UINT64, DT_FLOAT8_E5M2, DT_FLOAT8_E4M3FN]\n  device='GPU'; T in [DT_FLOAT]\n  device='DEFAULT'; T in [DT_INT32]\n  device='CPU'; T in [DT_UINT64]\n  device='CPU'; T in [DT_INT64]\n  device='CPU'; T in [DT_UINT32]\n  device='CPU'; T in [DT_UINT16]\n  device='CPU'; T in [DT_INT16]\n  device='CPU'; T in [DT_UINT8]\n  device='CPU'; T in [DT_INT8]\n  device='CPU'; T in [DT_INT32]\n  device='CPU'; T in [DT_HALF]\n  device='CPU'; T in [DT_BFLOAT16]\n  device='CPU'; T in [DT_FLOAT]\n  device='CPU'; T in [DT_DOUBLE]\n  device='CPU'; T in [DT_COMPLEX64]\n  device='CPU'; T in [DT_COMPLEX128]\n  device='CPU'; T in [DT_BOOL]\n  device='CPU'; T in [DT_STRING]\n  device='CPU'; T in [DT_RESOURCE]\n  device='CPU'; T in [DT_VARIANT]\n\n\t [[dynamic_seq2seq/decoder/decoder/LogicalOr]]"
     ]
    }
   ],
   "source": [
    "from ai_benchmark import AIBenchmark\n",
    "benchmark = AIBenchmark()\n",
    "results = benchmark.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metal_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
